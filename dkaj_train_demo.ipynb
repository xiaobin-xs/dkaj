{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98936dbb",
   "metadata": {},
   "source": [
    "Quick demo of the full DKAJ pipeline on the Framingham dataset:\n",
    "\n",
    "1. Load and preprocess data\n",
    "2. Fit SurvivalBoost\n",
    "3. TUNA warmup of a base neural net using SurvivalBoost leaves\n",
    "4. Train DKAJ on top of that warm-started net\n",
    "5. (Optional) Summary fine-tuning\n",
    "6. Evaluate using c-index and integrated brier score\n",
    "\n",
    "This is meant as an easy-to-read usage example, not a full experiment runner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60b9e970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datasets import load_dataset, LabTransformCR\n",
    "from models import (\n",
    "    SurvivalBoostWrap,\n",
    "    hist_gradient_boosting_classifier_apply,\n",
    "    create_base_neural_net_with_hypersphere,\n",
    "    tuna_loss,\n",
    "    DKAJ,\n",
    "    DKAJSummary,\n",
    "    DKAJSummaryLoss,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Set this flag if you want to run summary fine-tuning at the end\n",
    "do_summary_finetune = True  # from dkaj10.finetune_summaries = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33f7620",
   "metadata": {},
   "source": [
    "## Load and preprocess Framingham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8432e6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Framingham shapes:\n",
      "  X_train: (2482, 21)\n",
      "  X_val:   (621, 21)\n",
      "  X_test:  (1331, 21)\n",
      "  #events: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simple_data_splitting_val_ratio = 0.2\n",
    "fix_test_shuffle_train = True  # DEFAULT.fix_test_shuffle_train = 1\n",
    "\n",
    "(\n",
    "    X_full_train_raw_np,\n",
    "    Y_full_train_np,\n",
    "    D_full_train_np,\n",
    "    X_test_raw_np,\n",
    "    Y_test_np,\n",
    "    D_test_np,\n",
    "    features_before_preprocessing,\n",
    "    features_after_preprocessing,\n",
    "    events,\n",
    "    train_test_split_prespecified,\n",
    "    build_preprocessor_and_preprocess,\n",
    "    apply_preprocessor,\n",
    ") = load_dataset(\n",
    "    \"framingham\",\n",
    "    random_seed_offset=0,\n",
    "    fix_test_shuffle_train=fix_test_shuffle_train,\n",
    "    competing=True,\n",
    ")\n",
    "\n",
    "# Split original training into train/validation using config ratio\n",
    "X_train_raw_np, X_val_raw_np, Y_train_np, Y_val_np, D_train_np, D_val_np = train_test_split(\n",
    "    X_full_train_raw_np,\n",
    "    Y_full_train_np,\n",
    "    D_full_train_np,\n",
    "    test_size=simple_data_splitting_val_ratio,\n",
    "    random_state=0,\n",
    "    stratify=D_full_train_np,\n",
    ")\n",
    "\n",
    "# Build preprocessor on training data, apply to val/test\n",
    "X_train_np, preprocessor = build_preprocessor_and_preprocess(X_train_raw_np)\n",
    "X_val_np = apply_preprocessor(X_val_raw_np, preprocessor)\n",
    "X_test_np = apply_preprocessor(X_test_raw_np, preprocessor)\n",
    "\n",
    "print(\"Framingham shapes:\")\n",
    "print(\"  X_train:\", X_train_np.shape)\n",
    "print(\"  X_val:  \", X_val_np.shape)\n",
    "print(\"  X_test: \", X_test_np.shape)\n",
    "print(\"  #events:\", len(events))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb20be4e",
   "metadata": {},
   "source": [
    "## Fit SurvivalBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db65bc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 65.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted SurvivalBoost model.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "survboost_learning_rate = 0.05        # can choose from [0.01, 0.05, 0.1, 0.5]\n",
    "survboost_n_iter = 200                # can choose from [20, 100, 200]\n",
    "survboost_max_depth = 4               # can choose from [-1, 4, 8, 16]\n",
    "survboost_n_time_grid_steps = 128     # can choose from [64, 128]\n",
    "survboost_ipcw_strategy = \"kaplan-meier\"  # choose from ['alternating', 'kaplan-meier']\n",
    "survboost_random_seed = 146561020\n",
    "\n",
    "y_train_df = pd.DataFrame(\n",
    "    np.vstack([D_train_np, Y_train_np]).T,\n",
    "    columns=[\"event\", \"duration\"],\n",
    ")\n",
    "\n",
    "survival_boost = SurvivalBoostWrap(\n",
    "    n_iter=survboost_n_iter,\n",
    "    learning_rate=survboost_learning_rate,\n",
    "    max_depth=survboost_max_depth,\n",
    "    n_time_grid_steps=survboost_n_time_grid_steps,\n",
    "    ipcw_strategy=survboost_ipcw_strategy,\n",
    "    random_state=survboost_random_seed,\n",
    ")\n",
    "survival_boost.fit(X_train_np, y_train_df)\n",
    "\n",
    "print(\"Fitted SurvivalBoost model.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e4096a",
   "metadata": {},
   "source": [
    "## TUNA warmup (learn neural net to mimic SurvivalBoost kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc710bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_random_times_per_data_point = 10    # can choose from [5, 10]\n",
    "tuna_batch_size = 1024                \n",
    "tuna_n_layers = 2                     # can choose from [2, 4]\n",
    "tuna_n_nodes = 128                    # can choose from [64, 128]\n",
    "tuna_squared_radius = 0.1             # can choose from [0.1]\n",
    "tuna_learning_rate = 1e-3             # can choose from [0.01, 0.001]\n",
    "tuna_epochs = 10                      # small for quick demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0338099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build (time, x) features with random times, then compute SurvivalBoost leaves\n",
    "X_train_with_random_times_np = []\n",
    "X_val_with_random_times_np = []\n",
    "\n",
    "for _ in range(n_random_times_per_data_point):\n",
    "    rand_t_train = np.random.uniform(\n",
    "        Y_train_np.min(), Y_train_np.max(),\n",
    "        size=X_train_np.shape[0]\n",
    "    ).reshape(-1, 1)\n",
    "    rand_t_val = np.random.uniform(\n",
    "        Y_val_np.min(), Y_val_np.max(),\n",
    "        size=X_val_np.shape[0]\n",
    "    ).reshape(-1, 1)\n",
    "\n",
    "    X_train_with_random_times_np.append(np.hstack([rand_t_train, X_train_np]))\n",
    "    X_val_with_random_times_np.append(np.hstack([rand_t_val, X_val_np]))\n",
    "\n",
    "X_train_with_random_times_np = np.vstack(X_train_with_random_times_np)\n",
    "X_val_with_random_times_np = np.vstack(X_val_with_random_times_np)\n",
    "\n",
    "# Predict leaves for these (time, x) vectors\n",
    "survival_boost_leaves_train_np = hist_gradient_boosting_classifier_apply(\n",
    "    survival_boost.estimator_,\n",
    "    X_train_with_random_times_np,\n",
    "    n_threads=os.cpu_count() or 1,\n",
    ")\n",
    "survival_boost_leaves_val_np = hist_gradient_boosting_classifier_apply(\n",
    "    survival_boost.estimator_,\n",
    "    X_val_with_random_times_np,\n",
    "    n_threads=os.cpu_count() or 1,\n",
    ")\n",
    "\n",
    "# Aggregate leaves over random times: (n_samples, n_random_times * leaf_dim)\n",
    "n_train = X_train_np.shape[0]\n",
    "n_val = X_val_np.shape[0]\n",
    "n_times = n_random_times_per_data_point\n",
    "\n",
    "survival_boost_leaves_train_np = (\n",
    "    survival_boost_leaves_train_np\n",
    "    .reshape(n_times, n_train, -1)\n",
    "    .transpose(1, 0, 2)\n",
    "    .reshape(n_train, -1)\n",
    ")\n",
    "survival_boost_leaves_val_np = (\n",
    "    survival_boost_leaves_val_np\n",
    "    .reshape(n_times, n_val, -1)\n",
    "    .transpose(1, 0, 2)\n",
    "    .reshape(n_val, -1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "296b62f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare TUNA dataloaders: map x -> aggregated leaf encoding\n",
    "X_train_t = torch.tensor(X_train_np, dtype=torch.float32, device=device)\n",
    "X_val_t = torch.tensor(X_val_np, dtype=torch.float32, device=device)\n",
    "leaves_train_t = torch.tensor(survival_boost_leaves_train_np, dtype=torch.float32, device=device)\n",
    "leaves_val_t = torch.tensor(survival_boost_leaves_val_np, dtype=torch.float32, device=device)\n",
    "\n",
    "tuna_train_data = list(zip(X_train_t, leaves_train_t))\n",
    "tuna_val_data = list(zip(X_val_t, leaves_val_t))\n",
    "\n",
    "tuna_train_loader = DataLoader(tuna_train_data, batch_size=tuna_batch_size, shuffle=True)\n",
    "tuna_val_loader = DataLoader(tuna_val_data, batch_size=tuna_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f196943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base neural net on hypersphere\n",
    "num_input_features = X_train_np.shape[1]\n",
    "base_neural_net = create_base_neural_net_with_hypersphere(\n",
    "    num_input_features,\n",
    "    [tuna_n_nodes for _ in range(tuna_n_layers)],\n",
    "    squared_radius=tuna_squared_radius,\n",
    ").to(device)\n",
    "\n",
    "tuna_optimizer = torch.optim.Adam(base_neural_net.parameters(), lr=tuna_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02aa5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TUNA warmup ===\n",
      "[TUNA] epoch 001 | train_loss=0.1310 | val_loss=0.1201\n",
      "[TUNA] epoch 002 | train_loss=0.1134 | val_loss=0.0936\n",
      "[TUNA] epoch 003 | train_loss=0.0839 | val_loss=0.0589\n",
      "[TUNA] epoch 004 | train_loss=0.0499 | val_loss=0.0290\n",
      "[TUNA] epoch 005 | train_loss=0.0256 | val_loss=0.0222\n",
      "[TUNA] epoch 006 | train_loss=0.0234 | val_loss=0.0263\n",
      "[TUNA] epoch 007 | train_loss=0.0242 | val_loss=0.0185\n",
      "[TUNA] epoch 008 | train_loss=0.0156 | val_loss=0.0122\n",
      "[TUNA] epoch 009 | train_loss=0.0112 | val_loss=0.0132\n",
      "[TUNA] epoch 010 | train_loss=0.0126 | val_loss=0.0142\n",
      "Completed TUNA warmup.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TUNA warmup ===\")\n",
    "for epoch in range(tuna_epochs):\n",
    "    # Training\n",
    "    base_neural_net.train()\n",
    "    train_loss = 0.0\n",
    "    n_train_seen = 0\n",
    "\n",
    "    for X_batch, leaves_batch in tuna_train_loader:\n",
    "        outputs = base_neural_net(X_batch)\n",
    "        loss = tuna_loss(outputs, leaves_batch, device=device)\n",
    "        tuna_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        tuna_optimizer.step()\n",
    "        train_loss += float(loss) * X_batch.size(0)\n",
    "        n_train_seen += X_batch.size(0)\n",
    "\n",
    "    train_loss /= max(1, n_train_seen)\n",
    "\n",
    "    # Validation\n",
    "    base_neural_net.eval()\n",
    "    val_loss = 0.0\n",
    "    n_val_seen = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, leaves_batch in tuna_val_loader:\n",
    "            outputs = base_neural_net(X_batch)\n",
    "            loss = tuna_loss(outputs, leaves_batch, device=device)\n",
    "            val_loss += float(loss) * X_batch.size(0)\n",
    "            n_val_seen += X_batch.size(0)\n",
    "    val_loss /= max(1, n_val_seen)\n",
    "\n",
    "    print(f\"[TUNA] epoch {epoch+1:03d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")\n",
    "\n",
    "print(\"Completed TUNA warmup.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a8f09c",
   "metadata": {},
   "source": [
    "## DKAJ training (with warm-started neural net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7dab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "dkaj_n_durations = 128     # can choose from [0, 64, 128]; 0 means \"use all unique event times\"\n",
    "dkaj_batch_size = 1024\n",
    "dkaj_learning_rate = 1e-3  # can choose from [0.01, 0.001]\n",
    "dkaj_alpha = 0.001         # can choose from [0, 0.001, 0.01]\n",
    "dkaj_sigma = 1.0           # can choose from [0.1, 1]\n",
    "dkaj_beta = 0.25           # can choose from [0.25, 0.5]\n",
    "dkaj_min_kernel_weight = 1e-2\n",
    "dkaj_max_epochs = 20           # small for a quick demo\n",
    "\n",
    "ANN_max_n_neighbors = 128  # ANN_max_n_neighbors = [128]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7817acf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaobin/Desktop/dkaj/pycox/preprocessing/discretization.py:37: UserWarning: cuts are not unique, continue with 113 cuts instead of 128\n",
      "  warnings.warn(f\"cuts are not unique, continue with {len(cuts)} cuts instead of {num}\")\n"
     ]
    }
   ],
   "source": [
    "# Discretize times using LabTransformCR\n",
    "if dkaj_n_durations == 0:\n",
    "    # Use all unique non-censored event times, up to a cap (512) as in run_dkaj\n",
    "    mask = (D_train_np >= 1)\n",
    "    n_unique_times = np.unique(Y_train_np[mask]).shape[0]\n",
    "    if n_unique_times > 512:\n",
    "        print(\n",
    "            f\"Trying to use all training unique event times, but there are {n_unique_times} \"\n",
    "            \"unique event times. Using upper limit of 512 instead.\"\n",
    "        )\n",
    "        label_transform = LabTransformCR(512, scheme=\"quantiles\")\n",
    "    else:\n",
    "        label_transform = LabTransformCR(np.unique(Y_train_np[mask]))\n",
    "else:\n",
    "    label_transform = LabTransformCR(dkaj_n_durations, scheme=\"quantiles\")\n",
    "\n",
    "Y_train_discrete_np, D_train_discrete_np = label_transform.fit_transform(Y_train_np, D_train_np)\n",
    "Y_val_discrete_np, D_val_discrete_np = label_transform.transform(Y_val_np, D_val_np)\n",
    "time_grid_train_np = label_transform.cuts\n",
    "\n",
    "# Build dataloaders after discretization\n",
    "X_train_t = torch.tensor(X_train_np, dtype=torch.float32, device=device)\n",
    "Y_train_t = torch.tensor(Y_train_discrete_np, dtype=torch.int64, device=device)\n",
    "D_train_t = torch.tensor(D_train_discrete_np, dtype=torch.int32, device=device)\n",
    "dkaj_train_data = list(zip(X_train_t, Y_train_t, D_train_t))\n",
    "\n",
    "X_val_t = torch.tensor(X_val_np, dtype=torch.float32, device=device)\n",
    "Y_val_t = torch.tensor(Y_val_discrete_np, dtype=torch.int64, device=device)\n",
    "D_val_t = torch.tensor(D_val_discrete_np, dtype=torch.int32, device=device)\n",
    "dkaj_val_data = list(zip(X_val_t, Y_val_t, D_val_t))\n",
    "\n",
    "dkaj_train_loader = DataLoader(dkaj_train_data, batch_size=dkaj_batch_size, shuffle=True)\n",
    "dkaj_val_loader = DataLoader(dkaj_val_data, batch_size=dkaj_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b75b22e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate DKAJ model on top of base_neural_net\n",
    "dkaj_tau = np.sqrt(-np.log(dkaj_min_kernel_weight))\n",
    "\n",
    "dkaj_model = DKAJ(\n",
    "    base_neural_net,\n",
    "    device=device,\n",
    "    alpha=dkaj_alpha,\n",
    "    sigma=dkaj_sigma,\n",
    "    beta=dkaj_beta,\n",
    "    tau=dkaj_tau,\n",
    "    max_n_neighbors=ANN_max_n_neighbors,\n",
    "    dkn_max_n_neighbors=ANN_max_n_neighbors,\n",
    ")\n",
    "dkaj_loss = dkaj_model.loss\n",
    "dkaj_optimizer = torch.optim.Adam(base_neural_net.parameters(), lr=dkaj_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce8953d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DKAJ training ===\n",
      "[DKAJ] epoch 001 | train_loss=0.3418 | val_loss=0.3392\n",
      "[DKAJ] epoch 002 | train_loss=0.3383 | val_loss=0.3361\n",
      "[DKAJ] epoch 003 | train_loss=0.3358 | val_loss=0.3337\n",
      "[DKAJ] epoch 004 | train_loss=0.3335 | val_loss=0.3319\n",
      "[DKAJ] epoch 005 | train_loss=0.3312 | val_loss=0.3306\n",
      "[DKAJ] epoch 006 | train_loss=0.3299 | val_loss=0.3295\n",
      "[DKAJ] epoch 007 | train_loss=0.3286 | val_loss=0.3285\n",
      "[DKAJ] epoch 008 | train_loss=0.3269 | val_loss=0.3275\n",
      "[DKAJ] epoch 009 | train_loss=0.3255 | val_loss=0.3266\n",
      "[DKAJ] epoch 010 | train_loss=0.3231 | val_loss=0.3256\n",
      "[DKAJ] epoch 011 | train_loss=0.3220 | val_loss=0.3246\n",
      "[DKAJ] epoch 012 | train_loss=0.3197 | val_loss=0.3236\n",
      "[DKAJ] epoch 013 | train_loss=0.3175 | val_loss=0.3229\n",
      "[DKAJ] epoch 014 | train_loss=0.3152 | val_loss=0.3221\n",
      "[DKAJ] epoch 015 | train_loss=0.3132 | val_loss=0.3215\n",
      "[DKAJ] epoch 016 | train_loss=0.3118 | val_loss=0.3209\n",
      "[DKAJ] epoch 017 | train_loss=0.3105 | val_loss=0.3201\n",
      "[DKAJ] epoch 018 | train_loss=0.3083 | val_loss=0.3194\n",
      "[DKAJ] epoch 019 | train_loss=0.3074 | val_loss=0.3188\n",
      "[DKAJ] epoch 020 | train_loss=0.3063 | val_loss=0.3183\n",
      "Completed DKAJ training.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== DKAJ training ===\")\n",
    "for epoch in range(dkaj_max_epochs):\n",
    "    # Training\n",
    "    base_neural_net.train()\n",
    "    train_loss = 0.0\n",
    "    n_train_seen = 0\n",
    "\n",
    "    for X_batch, Y_batch, D_batch in dkaj_train_loader:\n",
    "        embeddings = base_neural_net(X_batch)\n",
    "        loss = dkaj_loss(embeddings, Y_batch, D_batch)\n",
    "\n",
    "        dkaj_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        dkaj_optimizer.step()\n",
    "\n",
    "        train_loss += float(loss) * X_batch.size(0)\n",
    "        n_train_seen += X_batch.size(0)\n",
    "\n",
    "    train_loss /= max(1, n_train_seen)\n",
    "\n",
    "    # Build ANN index & compute simple validation loss\n",
    "    dkaj_model.training_data = (\n",
    "        X_train_np.astype(\"float32\"),\n",
    "        (Y_train_discrete_np.astype(\"int64\"), D_train_discrete_np.astype(\"int32\")),\n",
    "    )\n",
    "    dkaj_model.train_embeddings = dkaj_model.predict(\n",
    "        X_train_np.astype(\"float32\"),\n",
    "        batch_size=dkaj_batch_size,\n",
    "    )\n",
    "    dkaj_model.duration_index = time_grid_train_np\n",
    "    dkaj_model.build_ANN_index()\n",
    "\n",
    "    base_neural_net.eval()\n",
    "    val_loss = 0.0\n",
    "    n_val_seen = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, Y_batch, D_batch in dkaj_val_loader:\n",
    "            embeddings = base_neural_net(X_batch)\n",
    "            loss = dkaj_loss(embeddings, Y_batch, D_batch)\n",
    "            val_loss += float(loss) * X_batch.size(0)\n",
    "            n_val_seen += X_batch.size(0)\n",
    "    val_loss /= max(1, n_val_seen)\n",
    "\n",
    "    print(f\"[DKAJ] epoch {epoch+1:03d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")\n",
    "\n",
    "print(\"Completed DKAJ training.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e629796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing CIFs on test set (demo)...\n",
      "CIFs shape: (2, 113, 1331)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing CIFs on test set (demo)...\")\n",
    "cifs = dkaj_model.predict_cif(\n",
    "    X_test_np.astype(\"float32\"),\n",
    "    batch_size=dkaj_batch_size,\n",
    "    numpy=True,\n",
    "    to_cpu=True,\n",
    ")\n",
    "cifs_np = np.array(cifs)  # shape: (n_events, n_durations, n_test)\n",
    "print(\"CIFs shape:\", cifs_np.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a91701a",
   "metadata": {},
   "source": [
    "## Optional summary fine-tuning (DKAJSummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6eea2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Summary fine-tuning ===\n",
      "[Summary] epoch 001 | loss=0.2191\n",
      "[Summary] epoch 002 | loss=0.2189\n",
      "[Summary] epoch 003 | loss=0.2190\n",
      "[Summary] epoch 004 | loss=0.2189\n",
      "[Summary] epoch 005 | loss=0.2189\n",
      "[Summary] epoch 006 | loss=0.2188\n",
      "[Summary] epoch 007 | loss=0.2186\n",
      "[Summary] epoch 008 | loss=0.2186\n",
      "[Summary] epoch 009 | loss=0.2185\n",
      "[Summary] epoch 010 | loss=0.2186\n",
      "Completed summary fine-tuning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if do_summary_finetune:\n",
    "    sumtune_learning_rate = 1e-3  # can choose from [0.01, 0.001, 0.0001]\n",
    "    sumtune_max_epochs = 10       # smaller for demo\n",
    "\n",
    "    # Use same alpha/sigma as main DKAJ\n",
    "    sumtune_alpha = dkaj_alpha\n",
    "    sumtune_sigma = dkaj_sigma\n",
    "\n",
    "    # Reuse discrete labels\n",
    "    Y_train_disc_np, D_train_disc_np = label_transform.transform(Y_train_np, D_train_np)\n",
    "\n",
    "    X_train_t = torch.tensor(X_train_np, dtype=torch.float32, device=device)\n",
    "    Y_train_t = torch.tensor(Y_train_disc_np, dtype=torch.int64, device=device)\n",
    "    D_train_t = torch.tensor(D_train_disc_np, dtype=torch.int32, device=device)\n",
    "    sumtune_train_data = list(zip(X_train_t, Y_train_t, D_train_t))\n",
    "    sumtune_train_loader = DataLoader(sumtune_train_data, batch_size=dkaj_batch_size, shuffle=True)\n",
    "\n",
    "    # Initialize summary network around current summary functions\n",
    "    init_summary_functions = dkaj_model.get_summary_functions()\n",
    "    summary_net = DKAJSummary(\n",
    "        dkaj_model,\n",
    "        init_summary_functions[0],\n",
    "        init_summary_functions[1],\n",
    "    ).to(device)\n",
    "    summary_loss = DKAJSummaryLoss(sumtune_alpha, sumtune_sigma)\n",
    "    summary_optimizer = torch.optim.Adam(summary_net.parameters(), lr=sumtune_learning_rate)\n",
    "\n",
    "    print(\"=== Summary fine-tuning ===\")\n",
    "    for epoch in range(sumtune_max_epochs):\n",
    "        summary_net.train()\n",
    "        epoch_loss = 0.0\n",
    "        n_train_seen = 0\n",
    "\n",
    "        for X_batch, Y_batch, D_batch in sumtune_train_loader:\n",
    "            with torch.no_grad():\n",
    "                embeddings = base_neural_net(X_batch)\n",
    "            event_hazards, overall_hazards = summary_net(embeddings)\n",
    "            loss = summary_loss(\n",
    "                event_hazards,\n",
    "                overall_hazards,\n",
    "                Y_batch,\n",
    "                D_batch,\n",
    "            )\n",
    "\n",
    "            summary_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            summary_optimizer.step()\n",
    "\n",
    "            epoch_loss += float(loss) * X_batch.size(0)\n",
    "            n_train_seen += X_batch.size(0)\n",
    "\n",
    "        epoch_loss /= max(1, n_train_seen)\n",
    "\n",
    "        # Update dkaj_model's summary functions from the summary net\n",
    "        (\n",
    "            exemplar_event_counts_new,\n",
    "            exemplar_at_risk_counts_new,\n",
    "            baseline_event_counts,\n",
    "            baseline_at_risk_counts,\n",
    "        ) = summary_net.get_exemplar_summary_functions_baseline_event_at_risk_counts()\n",
    "        dkaj_model.load_summary_functions(\n",
    "            exemplar_event_counts_new,\n",
    "            exemplar_at_risk_counts_new,\n",
    "            baseline_event_counts,\n",
    "            baseline_at_risk_counts,\n",
    "        )\n",
    "\n",
    "        print(f\"[Summary] epoch {epoch+1:03d} | loss={epoch_loss:.4f}\")\n",
    "\n",
    "    print(\"Completed summary fine-tuning.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4719ef",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "- Uses training data only to set the time grids\n",
    "- Computes Brier scores at the 0.25/0.5/0.75 quantile times\n",
    "- Computes integrated Brier score up to the 90th percentile of training event times\n",
    "- Computes the time-dependent concordance index (Ctd) per event and averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35733f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import KaplanMeierFitter\n",
    "from metrics import (\n",
    "    neg_cindex_td,\n",
    "    compute_brier_competing_multiple_times,\n",
    "    compute_ibs_competing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "796ea68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing CIFs on test set (demo)...\n",
      "CIFs shape: (2, 113, 1331)\n",
      "\n",
      "Evaluation horizons (quantiles based on training set):\n",
      "  q=0.25 -> t=2231.2500\n",
      "  q=0.50 -> t=4639.0000\n",
      "  q=0.75 -> t=6751.5000\n",
      "IBS integration from 1.0 to 7942.000000000002\n",
      "\n",
      "Evaluating event 1 (non-CVD Death)\n",
      "Evaluating event 2 (CVD Death)\n",
      "\n",
      "=== Test-set metrics (averaged over events) ===\n",
      "Average IBS (0 to 90th percentile): 0.0838\n",
      "Brier score at q=0.25 (t=2231.2500): 0.0513\n",
      "Brier score at q=0.50 (t=4639.0000): 0.0956\n",
      "Brier score at q=0.75 (t=6751.5000): 0.1370\n",
      "Average time-dependent concordance (Antolini Ctd): 0.6699\n",
      "\n",
      "Per-event Ctd:\n",
      "  non-CVD Death: 0.6386\n",
      "  CVD Death: 0.7012\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing CIFs on test set (demo)...\")\n",
    "events_pretty_names = ['non-CVD Death', 'CVD Death']\n",
    "cifs = dkaj_model.predict_cif(\n",
    "    X_test_np.astype(\"float32\"),\n",
    "    batch_size=dkaj_batch_size,\n",
    "    numpy=True,\n",
    "    to_cpu=True,\n",
    ")\n",
    "cifs_np = np.array(cifs)  # shape: (n_events, n_durations, n_test)\n",
    "print(\"CIFs shape:\", cifs_np.shape)\n",
    "print()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5. Evaluation: Brier scores, Ctd, IBS (training-based horizons)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "eval_horizon_quantiles = np.array([0.25, 0.5, 0.75])\n",
    "ibs_n_horizon_points = 100\n",
    "ibs_max_horizon_percentile = 0.9\n",
    "\n",
    "# Use ONLY the training set to define time horizons\n",
    "# Here we use the original full training split from load_dataset\n",
    "Y_all_train = Y_full_train_np\n",
    "D_all_train = D_full_train_np\n",
    "event_times_train = Y_all_train[D_all_train > 0]\n",
    "\n",
    "eval_horizons = np.quantile(event_times_train, eval_horizon_quantiles)\n",
    "tau = np.percentile(event_times_train, ibs_max_horizon_percentile * 100.0)\n",
    "ibs_integrate_horizons = np.linspace(\n",
    "    event_times_train.min(),\n",
    "    tau,\n",
    "    ibs_n_horizon_points,\n",
    ")\n",
    "\n",
    "print(\"Evaluation horizons (quantiles based on training set):\")\n",
    "for q, t in zip(eval_horizon_quantiles, eval_horizons):\n",
    "    print(f\"  q={q:.2f} -> t={t:.4f}\")\n",
    "print(\"IBS integration from\", float(event_times_train.min()), \"to\", float(tau))\n",
    "print()\n",
    "\n",
    "# Fit censoring distribution Kaplan–Meier on TRAINING split only\n",
    "censoring_kmf = KaplanMeierFitter()\n",
    "censoring_kmf.fit(\n",
    "    durations=Y_train_np,\n",
    "    event_observed=(D_train_np == 0).astype(int),\n",
    ")\n",
    "\n",
    "test_eval_brier_scores_all_events = []  # shape -> (n_events, n_eval_horizons)\n",
    "test_ibs_all_events = []                # shape -> (n_events,)\n",
    "test_cindex_td_all_events = []          # shape -> (n_events,)\n",
    "\n",
    "# Loop over each event type (1, 2, ...)\n",
    "for e_idx_minus_1, event in enumerate(events_pretty_names):\n",
    "    event_of_interest = e_idx_minus_1 + 1\n",
    "    print(f\"Evaluating event {event_of_interest} ({event})\")\n",
    "\n",
    "    # CIF for this event on test set: (n_durations, n_test)\n",
    "    cif_event_test = cifs_np[e_idx_minus_1]  # (n_durations, n_test)\n",
    "    n_durations, n_test = cif_event_test.shape\n",
    "\n",
    "    # Interpolate CIF at eval_horizons and IBS horizons\n",
    "    cif_eval_grid = np.empty((n_test, len(eval_horizons)))          # (n_test, n_eval_horizons)\n",
    "    cif_ibs_grid = np.empty((n_test, len(ibs_integrate_horizons)))  # (n_test, n_ibs_points)\n",
    "\n",
    "    for j in range(n_test):\n",
    "        cif_eval_grid[j, :] = np.interp(\n",
    "            eval_horizons,\n",
    "            dkaj_model.duration_index,\n",
    "            cif_event_test[:, j],\n",
    "        )\n",
    "        cif_ibs_grid[j, :] = np.interp(\n",
    "            ibs_integrate_horizons,\n",
    "            dkaj_model.duration_index,\n",
    "            cif_event_test[:, j],\n",
    "        )\n",
    "\n",
    "    # --- Brier scores at the 0.25 / 0.5 / 0.75 quantile times ---\n",
    "    eval_brier_scores = compute_brier_competing_multiple_times(\n",
    "        cif_values_grid=cif_eval_grid,     # (n_samples, n_timepoints)\n",
    "        censoring_kmf=censoring_kmf,\n",
    "        Y_test=Y_test_np,\n",
    "        D_test=D_test_np,\n",
    "        event_of_interest=event_of_interest,\n",
    "        time_horizons=eval_horizons,\n",
    "    )\n",
    "\n",
    "    # --- Integrated Brier Score (IBS) up to 90th percentile ---\n",
    "    ibs = compute_ibs_competing(\n",
    "        cif_values_grid=cif_ibs_grid,\n",
    "        censoring_kmf=censoring_kmf,\n",
    "        Y_test=Y_test_np,\n",
    "        D_test=D_test_np,\n",
    "        event_of_interest=event_of_interest,\n",
    "        time_horizons=ibs_integrate_horizons,\n",
    "    )\n",
    "\n",
    "    # --- Time-dependent concordance index (Antolini) ---\n",
    "    # Follow run_dkaj.py: use -CIF as \"survival-like\" scores on the IBS time grid\n",
    "    surv_for_ctd = -cif_ibs_grid.T  # shape (n_times, n_samples)\n",
    "    cindex_td = -neg_cindex_td(\n",
    "        Y_test_np,\n",
    "        (D_test_np == event_of_interest).astype(int),\n",
    "        (surv_for_ctd, ibs_integrate_horizons),\n",
    "        exact=False,\n",
    "    )\n",
    "\n",
    "    test_eval_brier_scores_all_events.append(eval_brier_scores)\n",
    "    test_ibs_all_events.append(ibs)\n",
    "    test_cindex_td_all_events.append(cindex_td)\n",
    "\n",
    "# Convert to arrays for easy summarization\n",
    "test_eval_brier_scores_all_events = np.array(test_eval_brier_scores_all_events)  # (n_events, n_eval_horizons)\n",
    "test_ibs_all_events = np.array(test_ibs_all_events)                              # (n_events,)\n",
    "test_cindex_td_all_events = np.array(test_cindex_td_all_events)                  # (n_events,)\n",
    "\n",
    "# Aggregate across events with equal weights (which is the criteria used in the paper for early stopping when evaluted on the validation set)\n",
    "avg_IBS = test_ibs_all_events.mean()\n",
    "avg_Brier_per_horizon = test_eval_brier_scores_all_events.mean(axis=0)\n",
    "avg_Ctd = test_cindex_td_all_events.mean()\n",
    "\n",
    "print(\"\\n=== Test-set metrics (averaged over events) ===\")\n",
    "print(f\"Average IBS (0 to 90th percentile): {avg_IBS:.4f}\")\n",
    "for q, t, b in zip(eval_horizon_quantiles, eval_horizons, avg_Brier_per_horizon):\n",
    "    print(f\"Brier score at q={q:.2f} (t={t:.4f}): {b:.4f}\")\n",
    "print(f\"Average time-dependent concordance (Antolini Ctd): {avg_Ctd:.4f}\")\n",
    "\n",
    "print(\"\\nPer-event Ctd:\")\n",
    "for event_name, ctd in zip(events_pretty_names, test_cindex_td_all_events):\n",
    "    print(f\"  {event_name}: {ctd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a6c498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conv_sota39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
